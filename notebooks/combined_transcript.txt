I should have a poll, but just show on hand, was that a hard quiz or a medium quiz? I guess the other question I wanted to say is, how many of you missed week one? Okay, and the real question for this quiz is, reading the materials that are posted on Canvas, were you able to answer this quiz? Okay, so we're going to have these quizzes that are going to be practicing with flavor. It states that when you walk away from this class, you should know how to answer. So it will be really hot, like foundations, concepts, terms that you will be able to understand. So if you're reading an article, you will be able to read the details. Today we're going to do two things. One is, as mentioned last lecture, I do want to use AI to generate the lecture notes. Because my slides are not going to have all the information we need. I'm going to still be sharing those, but today for example, we don't have a slideshow. And we're going to be doing some coding. But still, if there is an AI that's listening, it might be able to capture what we're discussing in the course. And it will generate some lecture notes that will be helpful. And then I can just put that on Canvas. Now there are tools that I could maybe open Zoom and get my Zoom AI subscription and turn it on. But this is an AI application class, so it's just fitting that we build our own. But also when we build our own, we're going to have really good controls about how the application is going to run. Like what kind of lecture notes do I want. And how long these lecture notes are. I don't know what Zoom AI holds when they're open. I would like to know. And I could use it, but I don't think it was designed for meetings, for example. If I open Zoom and it's important, it's going to do something, thinking it's a meeting. But we are in a lecture. There might be an application that does what we want to do, but we build AI applications. So it's fitting that we build our own app. Full disclaimer, we had the idea last lecture. So this is not something that I knew if it's going to work or not. And I find myself in this position every day where the question is like, can AI do that? And the best way to answer that question is like, let's find out. And you will see that we'll see if we can make AI do that for us. So a few things. Jane did record the lecture last time. And I'm going to try to play it. I need to change the output of this book. The audio? Yeah, audio output. Okay. So it is actually playing in the speakers. Whether I'm playing it on the laptop or on the speaker, it's a really, really bad quality audio. So when I play the audio, I'm like, I don't know. But we're going to use it anyway. So it's bad in a way that there is a lot of background noise from the AC. I'm not speaking loud enough. And I don't remember where was the mic. I'm recording too, so it's not so close. I'm walking around. So the audio is really bad. But we're going to use it anyway. So the model that does audio to text is different than the GBD4. It's called Whisper. This is one of the models. And Whisper is provided by OpenAI. So the first thing I wanted to check is, okay, how do I call the model? And also, is there a limitation on file size and stuff like that? So I went ahead and I tried to look for some information. I don't remember what I clicked. But obviously we're using, we want to use the best one. So we're going to use the large model. But there was some information somewhere. So, yeah, so there is a limitation on the file size. It's not a huge problem because I can just chop my file and send it. So, I mean, what's the worst thing that's going to happen is every time I use. So our file is 175 megabits. So it's a little bit larger than the minimum. We could do two things. We could use an audio software to reduce the quality. But it's already really bad quality, so we don't want to do that. So I thought, let's just chop it. And when we do that, we might miss the word here and there. What's really nice about AI is AI is really good about telling you what's here and there. So I'm not worried about that. So let's start coding. We have a library that I never used before. I looked it up, so there are probably other libraries that do audio processing. This one is from Tide Up. It's called Audio. Well, this particular module is called Audio Segment, and it does exactly that. So if we look at the code here, it's just going to read the input file. It's going to output the segments to the output file. And the segment length is in minutes. I did a quick test using iMovie to slice my audio and see, to get the 25 megabytes limit, how many minutes from my audio file every audio file will be different. So I figured out that 10 minutes is pretty safe. That will give us less than 25 megabytes of audio. And this library will do the rest. Another full disclaimer, ChadGBT wrote my function. I never used this library before. And, again, a lot of times you will see that Copilot or ChadGBT will give me code that doesn't work. In this instance, because this library has been probably there for 10, 15 years, so many slides sent over full articles. ChadGBT got it right. I probably got me a really good high quality code. It's not always the case. But I'll take it whenever I can. So the next step is to actually run this with MP3 audio. Another thing I wanted to try is, like, can I use a library that will split it at 10 minutes, but it will affect the pause, so it will try to split it at 10 minutes. I'm speaking in mid-flight. Actually, it was a mid-walk. I wanted it to pause and shift it maybe 200 milliseconds here or a second or whatever. And there is another library that does that. And it did work well. But this function grew in size. And the benefit was not huge because it's not really detecting.
an idea or a thought, you're just detecting your thoughts. So the model is not going to, GBT-4 is not going to have difficulties really filling a missing word in my sentence. So I opted to not complicate the code and just run this one. So I don't know if it ran the first time or not. But right now it's running. It's generating the audio segments. And this is called preprocessing right now. I have to do this step just because of the limitation that the model wants an audio file that is less than 35 feet in length. And you might end up having to do that. OK, so that is done. And we can look at the segments. They are generated. They're all here on my left panel. So the next step is, anybody have a next step in mind? OK. The next step is to iterate through these files and send them one at a time. Now, every time we send the file, we're going to get the, so the Whisper model, maybe I should explain a little bit. The Whisper model is not going to try to understand anything. It's going to transfer my thoughts into text. The Whisper model is a specialized washed language model. All that it does is transfer. It was trained on voice, text, voice, text, training set. That's all that it does. It does it very well. So every time I send the voice file to the Whisper model, it's going to come back with a new state. So that isn't going to generate a bunch of notes. It's going to just generate a transcript of what it's saying. And these models are on your phone. They are on your application. And any app you use with the microphone button, they're using a similar model. However, Whisper model is really better than the models you use on the phone and stuff, because this is a very large model. It's just that it's not so easily available for real-time conversion. But for a test like this, it's going to work really, really well. Yeah? Is it going to give you timestamps? Or is it going to just give you text? I don't know. Let's find out. All right, so the question was, is it going to give me also timestamps for the transcription? Or is it going to just give me text? And I'm not sure. So we can find out. We can look at the output of the model together. And we can find out. But just to give you kind of a what did I have to do, I just maybe said OpenAI Whisper API call, speech-to-text. And this is the quick start guide. This was enough for me. Another disclaimer, I tried to use Chair-GBT to give me the function that will iterate on the audio files and call Whisper. And it did give me a function that didn't work. And the reason is because OpenAI updated their Whisper model with a new model a few months ago. And GitHub's ProPilot or Chair-GBT really didn't have the new information, the training data. So they gave me the old code. And that didn't work. And they gave me a really old code because it didn't just warn me about duplications. It just didn't work. So to call the Whisper model, we need first to initialize our OpenAI client. And this code right here is just using the OpenAI client that is provided by OpenAI. And just giving it the key that I'm reading from the environment file. And I think if we have time today, we're going to just spend some time just making sure everybody's set up in your local machine about how to run these applications. So this is the main function that will take an audio file. And this will be a small audio file, a segment. And it will have the output file where it's going to output the text file. The main function from OpenAI is audio transcriptions create. Now, there might be other functions that we could try. We could look at the API documentation and see what else, what other services they're offering. I'm choosing Whisper 1, which is their latest and greatest model. And then basically what I'm doing is I'm opening the file. I'm passing it to Whisper OpenAI. And then I'm reading the text. So that's where we can put some debugging statement to answer your question about whether they give me also a timestamp or not. So we're going to try that together. So I'm calling this function on a loop because I'm looping in all my segments. But to run an experiment, what we're going to do is we're going to call the function. And we're going to pass the first segment. And for the output, we're going to just call this one a test output. It's going to be. And I'm going to just comment this out. I'm going to run this as is first. And it is saying that something is not correct. And I think because I didn't run the main function cell. And now it's taking my file, segment 1. And it's going to output a file that's called test. Now, I'm calling this file test. This is my function. Really, OpenAI is not involved in what I'm calling the file or whatever. It's just OpenAI. They're only concerned about the audio file that I'm reading from my file. And then what model to apply. And then they're giving me this object back. Now, you can see that there is a refresh here. I just feel like there must be a refresh somewhere here. OK, so live demo. Live demo. So there was an error. And it's saying the file directory didn't exist. And that's why we have this line of code right here. So we're going to run this file. Now, a better Python developer would have made this line of code inside the function. We're going to try again. OK, great. The transcription is generated. And we can take a look at it. So we can go to, we can just open it in Visual Studio. So we can do a board graph. So here's the transcription text from the recording. And what we want to do also, we just need to take a peek at what's inside this object. So I'm going to run this again. And now, we have to look at the message.
Okay, so it's completed again and I think I'm just trying to find where that statement went. It did go somewhere. Oh, it's going to go under the cell, because we are in jupyter notebook. And it didn't go, it's because we're not really running the cell. So this is a little bit tricky to do, but we're going to come back and look at that object. But what we can also do is just go to the API. And it's saying that the transcription actually, it looks like it doesn't have much. But we can take a look at this later, maybe if we have enough time, and we'll look at what other properties that object has. And it might have time stamps, which will be very useful and helpful. And this could be an exercise, an excellent exercise for you to go back and try to play with the code and try to look at what other properties are inside that object. Okay, so for our use case, we're going to take this testing code away. And we're going to actually just go ahead and run this code for all segments. We are repeating segment one. I'm going to delete this file, we don't need it anymore. And this is going to run for a little bit, because it looks like it's taking about 30 seconds per file. We have eight segments, it's going to take a few minutes. So we're going to look ahead and look at the next cell. So another question is, now we have text files with a transcript, what do we want to do next? I'll take a step back. What are we trying to do in this JupyterMod? Great, so fit it in the LLM and ask it to summarize. So we are not calling whisper anymore, we're going to call JupyterMod and ask it to summarize. One thing to note, so that's exactly what we want to do. When we take a step back, I'm trying to create a stack, really so we can use it in this class. It's going to take this recording, and it's going to generate the next lecture notes. And we're going to put them in that stack. So the next step is we're going to pass these to the large language model and ask it to summarize, but really more specifically, we're going to ask it to do something else. We could just get a summary and we can put it there, but I think we could do a little bit more with that summary. And we could try both. There is just a small, minute detail that these text files are one file at a time. So we could either merge these files and save them in one file and then read it and pass it to the large language model. But we're going to read it anyway, so what I'm going to do in the next cell is I'm going to read one file, making sure that I'm reading one file at a time, but making sure I'm reading it in the right order. That's important because I don't want to read the files in the wrong order. And then I'm going to pass that entire text to the large language model. So you can see here that this is guaranteeing that we're reading one file at a time in the right order. So if you're not familiar with Python, this is a for loop that's going to count 1, 2, 3, 4, 5, 6, 7, 8, 9. And every time it counts, I'm going to get the file that is called segment underscore the number dot txt. I'm going to load the file, and I'm going to add the content of that file to a variable here called all content. This function can be written in so many different ways. So I'm not teaching you optimized code. I'm sure you can write it in a better way. I'm just going to focus on what we're trying to do here, but also sometimes just looking into these details are really important. So if we didn't pay attention about the order of the files, and we started just reading the files, these files could be sorted by size, by date, and you just want to make sure some of these details are important when you're building the application. Okay, it looks like we have all our files generated. So we're going to read the entire file, or all the files, and this is very quick because there is no API calls, nothing. And then just as a sanity check, let's make sure we read the entire thing, and it looks like this particular one has 42,000 characters. So we have a variable called combined content that has the entire lecture transcript. Okay, so we've done this before, chat gbt open AI, as a kind of a chat completion function that if we pass the model, and if we pass the body of the messages, it will give us the answer just like it does when you're using the UI. So here I'm going to write the prompt, and instead of saying summarize, I really wanted to generate a very well structured lecture note. So that is the system prompt. Now system prompts are optional, you could use them, I could have said the same thing as a user, and it will act almost the same way. If you don't provide a system prompt, the default system prompt for these models is UR and helpful AI systems. If you want to give it more of a character, charisma, you can add a system prompt. And then, so I added a system prompt telling it what I want it to do, and then I'm acting as a user. So when I speak to it in a system prompt, I'm more the creator of the AI, and when I speak to it as a user, it's more of what the user is asking the AI to do. But again, if the user is also asking it what to do from the beginning of the message, the AI will do that anyway, right? So this can go in here as well. Okay, so I'm asking it to generate the lecture notes from the transcript, and I have my content that I will pass to this function. It will get me the content of the response, remember this response will have other information. And I'm going to call the function right here. And the rest of the code is just writing it into a text file. Okay, who's ready? There was no streaming, right? So unfortunately, we have to wait until the whole thing is generated. But it was generated, so. So that was not the full output, but we can generate the full output. Actually, this is.
Okay, sorry, so that was the full output, it's just not displayed. And this next cell is just slightly different prompt. Here it's saying complete lecture notes. Here it's saying generate detailed lecture notes. But it's the same thing, so let's look at the output. So this is the result. And if you were here on Tuesday, you can validate this together. So we talked about when we were kind of celebrating that we built a chatbot, and we were saying, like, this is actually a lot of startups are trying to do, because this is a private chatbot, because you're talking directly to the API, you're storing the conversation in your application. We also talked about streaming. That was the first thing we did. And we did compare two chatbots about streaming. Then we kind of looked at stream. We talked about streamlet. We did a hands-on activity where we were talking about session state and variables, and so on and so on. And this is fantastic, because, like, really the lecture wasn't really structured. It was like this one. It was just bumbling around. And now I have a really well-structured lecture note. And we maybe paid, I don't know, five cents for this. We can look at it. Generally this one is pretty cheap. And how many lines of code was that? Not a lot. I mean, should we just package this, call it other AI? Start somewhere. We can have this other AI, and they charge you $25 to upload this file. That's it. I mean, other AI is not using their whisper. They're using open AI. That's something to keep in mind. We have GPT. We have Cloud. We have everybody is using GPT and Cloud. Any AI company, that's what they're using. They're not building special models that do better or anything, because there is no better models. And what's funny is these models are good at everything, so a lot of people are saying, oh, I'm going to find a model that's really good about medical practice. They fail, because it – so when you try to teach a model about one thing, it doesn't become smart enough. That's not surprising. We tell people take art, take math, take different courses. Even if you're trying to focus on computer science, diversify your courses. It's the same. It's like diversifying knowledge makes you smarter. You take knowledge from one domain. You apply it to another domain. That's why today the large language models, the generalized that learn about everything, they are better than any even large language model that is trained about like one specific topic. The generalized model beats the specific topic model by a lot, and that's because it's just smarter. It's a well-ground model. Okay, so we have an application that does generate lecture notes. I'm going to try and experiment here that I haven't tried yet. By the way, again, this was something that I – I just didn't want to try the whole thing in class, but I really tried this whole application for the first time yesterday night. Just to give you an idea that you, too, if you have any idea, just go and do it. It's not a lot of lines of code. Okay, so what if I want AI to do me a favor, and I want the AI to really make my content better. My lecture structure, maybe it's not perfect. Maybe I'm not saying enough examples. I'm going to just ask AI, in retrospect, after we're done here, if there is some ideas to add to the lectures to make it better, we can do that. Here we're very dynamic. I can answer questions. We can talk. We can go in different directions. After we're done, that is the lecture. If somebody wants to review it later, is it missing something? We can do that. It would be smart if I tried to say everything I want to say today and make the AI review it for me first, but I'm not sure exactly what we're going to talk about, but we're going to do it in retrospect. I'm going to change the prompt and add more to it. We're going to say something like, thanks, maybe, but I don't want to say thanks. I'm going to say, feel free to add any. That's good. Let's do any examples to enrich the content. Would be appreciated, sure, but I'm going to have to enrich the content. Here's where I can get the help of somebody who is expert with educational techniques, so I can be specific. I can maybe use thought-provoking dialogues to engage the reader. Let's try it right now. I think it's done. This is really, really fast. I'm going to just do everybody a favor. I'm going to delete the other notes. We don't need that. I'm going to go to lecture notes. I'm going to just skim through it quickly. It did do something different. It's asking takeaways and example dialogues. Let's see if it understood what I'm trying to do. It's kind of giving me feedback, and that wasn't my question. I'm going to go back to my prompt. Feel free to add additional information to the lecture notes for the reader. I'm going to say fill in any missing gaps to make the lecture complete. Examples to enrich the content. Let's remove this because maybe I'm confusing it. Let's take a look at the output. I'm going to just refresh this. Let's just take a look. Privacy and chat box. Streaming feature. It's adding these examples. It's trying to add stuff that I asked it to do. It's setting this entire practical example that wasn't there. It's kind of restructuring the conversation about the challenge, proposed solution, detailed approach.
So it's doing something, and I might spend time to just review it and see if this is what I want, and then I can go and tweak my prompt. And honestly, the first thing that I would do is, like, if I really want to use this prompt, I'm going to take this prompt, and I'm going to go to the AI, and I'm going to say, help me write a prompt for the LM to generate really good lecture notes based on the most advanced teaching techniques. Here's my draft. So my draft is a good starting point, which it is. That's true. It's just a starting point. And this is what it's suggesting that we do. You can see this is much better than what I was trying to say. And I'm going to take this whole code, and I'm going to base this prompt. I'm going to run this one more time. In practice, I'm going too fast here, because this requires me to think about my goals and stuff like that. So I would read that response from the AI about the prompt and modify it a little bit if I want to. But here, we're just trying. So you can see that the new lecture notes, it has a summary and learning objectives. It has an introduction. It has the goals. It has the hands-on development of what we did. And then it's talking about the memory. And it's really breaking the lecture actually how I should have approached the lecture in the first place. But you can see that it's really now rich. It's long. If I paste this, I'm going to review it before I post it onto Canvas. But I'm going to review this prompt. I might just wait on it a little bit more. But that's the process that I'm going to go through. And then I'm going to say, okay, this is unsatisfied. I'm going to use it in every lecture. What I can do is I can build an application using Streamlit or any other UI. I can publish it, and I can call it lecture note AI transcript. And I will allow people to upload their MP3. The code will chop it. Maybe I need to modify that code to try to understand the size of the file. Right now it doesn't, but it chops it for 10 minutes. Maybe 10 minutes will work. It doesn't matter. I would do some error handling. Maybe I will charge them money first or maybe put it for free. It doesn't matter. And then people will upload it, and they will see that this is the best AI transcript app they use. Because it's generating from full audio recording. It's generating this very well-structured lecture note. And students could use it for other classes as well. Why not? The idea here is the user doesn't know what was in prompt. So that's why prompt engineering became a thing. Because prompt engineering is powerful. Now, honestly, before I figured out the trick that I can ask AI to write me the prompt, it was hard. It was hard to come up with a really nice prompt. It takes time. It takes effort. And you need to really think about it. What happened is when Entropic released their 3.5, a lot of the training data was about prompt engineering. So the model understood what prompt engineering is. So I just have to tell it what I want, and it will give me this really nice prompt. And this prompt has concepts in it that was, you know, there are a few papers that were published about, you know, structure, thinking about it. We read that prompt in detail. Everything in that prompt actually might have an entire research paper about why this works with large language models. So don't underestimate what we did. We are lucky we can take that shortcut. Maybe we were holistic. I don't know. But before Claude understood what prompt engineering is, there were jobs, many, many job openings. It's prompt engineering in the big companies. We're talking hundreds and hundreds of jobs that pay $850K and more for prompt engineering. And though I don't know how widely understood that you can use the model for prompt engineering yet, so a lot of people might still have their jobs, but it's just something to think about. But prompts are really, really important. And we kind of did it. I'm actually going to go and read the prompt because I do want to make sure that we appreciate what this large language model gave us here. So now, my prompt wasn't terrible. Like, you can see that I did ask it when it's writing a prompt. I did kind of guide it to not just give me a prompt, but actually reference everything it's learned about teaching techniques and what's important for the students to learn. So that was key. So there is a clear structure, main topics, subtopics, key points, a brief summary, and learning objective at the beginning. These are all really good teaching techniques because I asked you to reference those. Use bullet points. Incorporate visual elements. I don't know if it did that because right now, this application I'm using doesn't support multi-modal output. Otherwise, ChargeGBD 4 can create drawings. Highlight key. The list goes on and on. You can see here, it's like, check your understanding section. So all of that went into our prompt, and that's why we got this really nice lecture note. Really from nothing. I mean, the audio file is really, really not the best quality audio file. Okay, any questions? Great question. Can we add external resources? That's a tricky answer. So, yes, I think we can. But hallucination is a problem. Now, because as we go in the course, you will understand that a little bit more. The AI right now I'm talking to doesn't have Internet access. So if I ask it for a reference about a blog post or anything, it's going to try to give me one. But it can't validate the link. Now, it's going to try to give me one, but remember, LLNs don't think about what they're going to do. They're generating the next likely character or token. Now, this is going to be really tricky for the LLN to get that URL and remember. It's actually not going to remember. The only reason it might give me links that will work is because those links are so popular that it had to read them over and over in its training data, where it can't predict what actually the link is. That's what happened to the famous attorney like 12 months ago. So if you missed it, it was this attorney that went to court and presented the case. And the clerk and basically everybody just started to find something really odd. I mean, the argument was perfect. It's like, this is my client's case. This is the precedent case.
there up he thought these because me and just like yet all I'm going to try to test the links. I think some links are going to be weird because they're popular, but some links we'll see. It will be fun. So we're going to go back to our prompt and we're going to say here at the end add at least five references for similar or for relative blog posts. Now I'm going to make it harder for the AI and kind of almost force you to give me specific websites, which will make it a little bit harder. Okay, so we have our regular results. We're going to open the full results. All right, so we have a few medium blogs. The first one is, let's play a game, the first one is Streamlight for Effective Chatbot Development. Do you think this is a real or fictional blog? Okay, real blog, raise your hand. I'm going to raise my hand, I think. I think this is common. Okay, actually I'm going to lower my hand. Yeah, do you see that? A lot of them have the ad user, so they're all fake. So what if I don't force it to give me medium blogs? What if I don't force it to give me blogs? What if I say five references for relative materials for reading? But it is not the list. Some of them work. It is a major problem, it's a major limitation, but there is something we can do about it. And if you asked the question 18 months ago, there wasn't something we can do about it. Okay, so let's look at the new, all right, so now we're getting papers. We're actually getting a paper that is large language models, a few shot training, and it's really insisting to giving us details about the archive number. So it's really confident about this. It's not just giving us a title, which maybe it remembered. So let's play a game with only the first entry. Is this real or is this AI? I'm going to say real. I'm going to trust it this time. Is it AI? So we can break it to two questions. Does the paper exist? And then is the archive number correct? I'm going to say, let's find out. So let's first of all, find the paper. I think the paper does exist. I have a feeling that the paper is right. So the paper is correct. And the archive, I think here, 2005, 14, 165. Oh, look at that. That's accurate. You can trust it. Trust AI. So yeah, that's great. That's one. I don't know. I mean, maybe the others. How about this one? It's also open AI. So it's interesting. It says beta. I don't think it's beta anymore, but it thinks it's beta. So it's changed the platform, but open AI did do the redirect, so it wasn't a broken link. Yeah, so do you want me to keep the optional references in the prompt and post it on Canvas? And, you know, put a disclaimer that these might be fictional, but if they exist, they might be useful. So I'm going to leave them just for fun. Just to waste your time sometimes. So I'm going to leave them. Just use your radar before you try to find it and waste your time. So yeah, this was kind of an unplanned session. I wasn't actually planning to talk about this work at all, but because we need this to generate any lecture notes, we'll go to Canvas. Now we have it. So next lecture, we're going to go back and continue talking about, we're talking about client training. The reason we're talking about client training is because it gives us some properties outside of the box, so we don't have to code a lot. We can just reuse it. And we are going to start to talk about RAG, which is Retrieval Augmented Generation. And that's when we start to talk about presenting data to the AI that it needs to give you a better answer. Let's see. So for the rest of the time, I'm just making sure this clock is correct. So we have about five minutes. So I want to just talk.
So I wanted to actually, so the repo is public, so you can fork it or clone it. Does anybody know how to use GitHub? If you haven't used GitHub before, find Jing or I, and we will help you. The feel-free-to-fork-it, if you do that, I don't know if you can make it private after you fork it. Just a reason, so I'm going to be sharing the OpenAI key with everybody. Just keep that safe. It's going to be, I might give you each your individual key, so if one key leaks, like I don't want to make sure, like I'm not going to send the new key for everybody, so I'm going to just replace you. And also this will tell me if you're mining, I mean you can't mine with one, but if you're doing some shady thing with the key, I'll be able to tell. So I'm going to do that. Have Visual Studio Code set up, have Docker set up. Run the code, if you've run into it. This is the time that we need to start like having everybody within a devolved environment because soon we're going to start to devolve. But I also want you to go back to these applications, customize them. Like we just talked about even what is Whispers getting us back? Is it giving us timestamps? And if it is, would you do anything with it to make the lecture notes better? If you do, just let me know and I will add your code, and from now on we'll use that. So if you come up with any idea. If you improve the prompt, also do that. Now we changed it and you say I can improve the prompt, so there's not so much room to improve it there. But otherwise we could have iterated with the prompts. And maybe I should have kept that secret for a little bit. If you run into any problem, Docker isn't installing, the code isn't running, try two things. There are the notebooks, the Jupyter notebooks, that are just for learning purposes that we just used today. Make sure you can run the cells, and then there is the Streamlet app files that you have to go and say Streamlet run application and make sure you can have your web application. We're going to continue building sometimes notebooks, and sometimes applications. You can take this app and make it an application. So you can take this notebook and make it an application. And you are free to publish it somewhere and ask for credit cards. I do believe that you can get credit cards. Just put it there or maybe give it. But just think, if you're doing anything like that and you're opening it to the public, just make sure you're using your own OpenAIT, not mine. Because it does cost money. So if you're giving them for free and putting donate button next to it, I'm paying for it, you're getting the donation. So don't do that. Okay. Thank you. Thank you, everyone. We're going to try to have 10 quizzes. I'll give you a heads up if we're having a quiz, but it's going to be similar.